{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import sys\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import count, avg\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import NaiveBayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
      "| Id| ProductId|        UserId|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|\n",
      "+---+----------+--------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|\n",
      "|  2|B00813GRG4|A1D87F6ZCVE5NK|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|\n",
      "|  3|B000LQOCH0| ABXLMWJIXXAIN|                   1|                     1|    4|1219017600| Delight says it all|\"This is a confec...|\n",
      "|  4|B000UA0QIQ|A395BORC6FGVXV|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|\n",
      "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|\n",
      "+---+----------+--------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# reading data\n",
    "df = sqlContext.read.csv(\"/Users/navyarao/Downloads/Amazon_reviews.csv\",header=True)\n",
    "#df = sqlContext.read.csv(\"/Users/Downloads/AmazonReviews_sample.csv\",header=True)\n",
    "print(df.show(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating new column \"status\" based on score. If score is lessthan or equal to 3 then considering the status as positive and if the score is greaterthan 3 then considering the status as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+--------------------+----------------------+-----+----+-------+----+\n",
      "| Id|ProductId|UserId|HelpfulnessNumerator|HelpfulnessDenominator|Score|Time|Summary|Text|\n",
      "+---+---------+------+--------------------+----------------------+-----+----+-------+----+\n",
      "|  0|        0|     0|                   0|                     0|    0|   0|      0|   0|\n",
      "+---+---------+------+--------------------+----------------------+-----+----+-------+----+\n",
      "\n",
      "+-----+--------------------+\n",
      "|Score|                Text|\n",
      "+-----+--------------------+\n",
      "|    5|I have bought sev...|\n",
      "|    1|\"Product arrived ...|\n",
      "|    4|\"This is a confec...|\n",
      "|    2|If you are lookin...|\n",
      "|    5|Great taffy at a ...|\n",
      "|    4|I got a wild hair...|\n",
      "|    5|This saltwater ta...|\n",
      "|    5|This taffy is so ...|\n",
      "|    5|Right now I'm mos...|\n",
      "|    5|This is a very he...|\n",
      "|    5|I don't know if i...|\n",
      "|    5|One of my boys ne...|\n",
      "|    1|My cats have been...|\n",
      "|    4|good flavor! thes...|\n",
      "|    5|The Strawberry Tw...|\n",
      "|    5|My daughter loves...|\n",
      "|    2|I love eating the...|\n",
      "|    5|I am very satisfi...|\n",
      "|    5|Twizzlers, Strawb...|\n",
      "|    5|Candy was deliver...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+----------+--------------+--------------------+----------------------+-----+----------+--------------------+--------------------+--------+\n",
      "| Id| ProductId|        UserId|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|  Status|\n",
      "+---+----------+--------------+--------------------+----------------------+-----+----------+--------------------+--------------------+--------+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|Positive|\n",
      "|  2|B00813GRG4|A1D87F6ZCVE5NK|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|Negative|\n",
      "|  3|B000LQOCH0| ABXLMWJIXXAIN|                   1|                     1|    4|1219017600| Delight says it all|\"This is a confec...|Positive|\n",
      "|  4|B000UA0QIQ|A395BORC6FGVXV|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|Negative|\n",
      "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|Positive|\n",
      "+---+----------+--------------+--------------------+----------------------+-----+----------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "df.select([c for c in df.columns if c in ['Text','Score']]).show()\n",
    "df = df.withColumn(\n",
    "    'Status',\n",
    "    f.when((f.col(\"Score\") >= 3), \"Positive\")\\\n",
    "    .otherwise(\"Negative\")\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------+------------------+--------------------+----------------------+------------------+--------------------+--------------------+--------------------+--------+\n",
      "|summary|                Id|    ProductId|            UserId|HelpfulnessNumerator|HelpfulnessDenominator|             Score|                Time|             Summary|                Text|  Status|\n",
      "+-------+------------------+-------------+------------------+--------------------+----------------------+------------------+--------------------+--------------------+--------------------+--------+\n",
      "|  count|             65534|        65533|             65533|               65533|                 65533|             65533|               65533|               65533|               65533|   65534|\n",
      "|   mean| 32767.99436924908|2.734888454E9|              null|  1.6161170707887629|     2.096882486686097|4.1492988265454045|1.2949877542490044E9|                null|                null|    null|\n",
      "| stddev|18917.901630469427|          0.0|              null|   5.372528378566806|     6.052316087026061|1.3256772593475714| 4.778423931105594E7|                null|                null|    null|\n",
      "|    min|                 1|   2734888454|#oc-R119LM8D59ZW8Y|                   0|                     0|                 1|          1067040000|      ! Mmmmmmmmmm !|               Grape|Negative|\n",
      "|    max|              9999|   B009WSNWC4|     AZZMO52V8WZ68|                  97|                    99|                 5|           962236800|�:::D:::� �:::E::...|~<br />This is a ...|Positive|\n",
      "+-------+------------------+-------------+------------------+--------------------+----------------------+------------------+--------------------+--------------------+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n",
    "df.describe().show()\n",
    "type(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive reviews have large count than negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|  Status| count|\n",
      "+--------+------+\n",
      "|Positive|484549|\n",
      "|Negative| 83905|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Status').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+--------------------+--------------------+----------------------+-----+----------+-------------------+--------------------+--------+\n",
      "|    Id| ProductId|            UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|            Summary|                Text|  Status|\n",
      "+------+----------+------------------+--------------------+--------------------+----------------------+-----+----------+-------------------+--------------------+--------+\n",
      "|290941|B005HG9ESG|#oc-R1GKUU1PTIDIZ7|\"Barbara Rhoades ...|                   0|                     0|    5|1343779200|       Better water|Water is water, r...|Positive|\n",
      "|291020|B005HG9ESG|#oc-R2F3I37IKVM0H0|\"Addison Dewitt \"...|                   0|                     3|    3|1341187200|Clear, Cool, Water?|When I was growin...|Positive|\n",
      "| 83696|B005ZBZLT4| #oc-R6LF0WYR2C9SB|           brown1829|                   1|                     8|    1|1332633600|        Worst Ever!|Thought I was get...|Negative|\n",
      "| 36396|B004CLCEDE|    A10103MJIKKIFE|               Dozer|                   1|                     1|    5|1345680000|               OMG!|I ordered the pro...|Positive|\n",
      "|437837|B005A1LGIY|    A108XP24UESKSV|\"E. Kay \"\"eclecti...|                   0|                     0|    4|1320364800| A flavorful drink!|\"I am a long-time...|Positive|\n",
      "+------+----------+------------------+--------------------+--------------------+----------------------+-----+----------+-------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['UserId', 'ProfileName','Time','Text']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we are tokenizing the text, preprocessing and removing stopwords from the text. And converting tokens to count vectors and building TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "Tokenizer_regex = RegexTokenizer(inputCol=\"Text\", outputCol=\"all_words\", pattern=\"\\\\W\")\n",
    "# stop words removal\n",
    "list_stopwords_additional = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n",
    "    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n",
    "    u'can', 'cant', 'come', u'could', 'couldnt', \n",
    "    u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n",
    "    u'each', \n",
    "    u'few', 'finally', u'for', u'from', u'further', \n",
    "    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n",
    "    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n",
    "    u'just', \n",
    "    u'll', \n",
    "    u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n",
    "    u'no', u'nor', u'not', u'now', \n",
    "    u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n",
    "    u'r', u're', \n",
    "    u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such',\n",
    "    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', \n",
    "    u'under', u'until', u'up', \n",
    "    u'very', \n",
    "    u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', \n",
    "    u'y', u'you', u'your', u'yours', u'yourself', u'yourselves'] \n",
    "\n",
    "Remove_stopwords = StopWordsRemover(inputCol=\"all_words\", outputCol=\"stopwords_removed\").setStopWords(list_stopwords_additional)\n",
    "# words count\n",
    "count_Vectors = CountVectorizer(inputCol=\"stopwords_removed\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "#label generation\n",
    "label_Target = StringIndexer(inputCol = \"Status\", outputCol = \"label\")\n",
    "#TF\n",
    "TF = HashingTF(inputCol=\"stopwords_removed\", outputCol=\"TF\", numFeatures=10000)\n",
    "#idf\n",
    "idf = IDF(inputCol=\"TF\", outputCol=\"features\", minDocFreq=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|  Status|           all_words|   stopwords_removed|            features|label|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|Positive|[i, have, bought,...|[bought, several,...|(10000,[1,2,7,11,...|  0.0|\n",
      "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|Negative|[product, arrived...|[product, arrived...|(10000,[7,106,120...|  1.0|\n",
      "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|Positive|[this, is, a, con...|[confection, arou...|(10000,[42,63,122...|  0.0|\n",
      "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|Negative|[if, you, are, lo...|[looking, secret,...|(10000,[2,8,40,47...|  1.0|\n",
      "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|Positive|[great, taffy, at...|[great, taffy, gr...|(10000,[3,24,275,...|  0.0|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[Tokenizer_regex, Remove_stopwords, count_Vectors ,label_Target])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df)\n",
    "final = pipelineFit.transform(df)\n",
    "final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into 75 percent train and 25 percent test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set seed  for train test split\n",
    "(trainData, testData) = final.randomSplit([0.75, 0.25], seed = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Count\n",
      "425888\n",
      "Test Data Count\n",
      "142566\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data Count\")\n",
    "print(str(trainData.count()))\n",
    "print(\"Test Data Count\")\n",
    "print(str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.classification.BinaryLogisticRegressionTrainingSummary at 0x5b91400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|                Text|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|I have bought sev...|[0.86521491951716...|  0.0|       0.0|\n",
      "|This is a very he...|[0.91002016546519...|  0.0|       0.0|\n",
      "|I fed this to my ...|[0.63498899727519...|  1.0|       0.0|\n",
      "|I have to admit, ...|[0.87806296483908...|  1.0|       0.0|\n",
      "|I love this noodl...|[0.88291013280326...|  0.0|       0.0|\n",
      "|That's pretty muc...|[0.82073464319410...|  0.0|       0.0|\n",
      "|These are very go...|[0.91153753318420...|  0.0|       0.0|\n",
      "|Very spicy packag...|[0.85891053323600...|  0.0|       0.0|\n",
      "|Delicious, awesom...|[0.90270526120574...|  0.0|       0.0|\n",
      "|Always good.  I d...|[0.88363590578553...|  0.0|       0.0|\n",
      "|It's quick to coo...|[0.89920111945357...|  0.0|       0.0|\n",
      "|the deliver is on...|[0.84963181637577...|  0.0|       0.0|\n",
      "|\"Love this soup! ...|[0.91999493629266...|  0.0|       0.0|\n",
      "|You get what you ...|[0.84900963889324...|  0.0|       0.0|\n",
      "|Good for a quick ...|[0.90660477928869...|  0.0|       0.0|\n",
      "|\"These are excell...|[0.95473263457641...|  0.0|       0.0|\n",
      "|I love this compa...|[0.87378420744678...|  0.0|       0.0|\n",
      "|One box of Shin N...|[0.78292159493106...|  1.0|       0.0|\n",
      "|Man this stuff is...|[0.88480033138307...|  0.0|       0.0|\n",
      "|One of my favorit...|[0.91180111353015...|  0.0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Train Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8444717631829979"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train = lrModel.transform(trainData)\n",
    "predictions_train.select(\"Text\",\"probability\",\"label\",\"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Train Accuracy\")\n",
    "evaluator.evaluate(predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|                Text|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|We have a 7 week ...|[0.92604458735751...|  0.0|       0.0|\n",
      "|The taste is grea...|[0.88326052879249...|  0.0|       0.0|\n",
      "|This is the best ...|[0.89501928224742...|  0.0|       0.0|\n",
      "|\"I don't see how ...|[0.90249562227259...|  0.0|       0.0|\n",
      "|I purchased this ...|[0.91021299920427...|  0.0|       0.0|\n",
      "|bought 2 package ...|[0.83423704706643...|  0.0|       0.0|\n",
      "|Shin Ramyun (sinc...|[0.86679681965417...|  0.0|       0.0|\n",
      "|I received Nongsh...|[0.86394863198892...|  0.0|       0.0|\n",
      "|These are perhaps...|[0.91557649654691...|  0.0|       0.0|\n",
      "|\"RECONSIDER THIS ...|[0.63917590959691...|  1.0|       0.0|\n",
      "|I really love thi...|[0.89871970167112...|  0.0|       0.0|\n",
      "|This is the most ...|[0.76614312816563...|  0.0|       0.0|\n",
      "|These noodles rem...|[0.96134203086638...|  0.0|       0.0|\n",
      "|They're not for t...|[0.88808134375812...|  0.0|       0.0|\n",
      "|I gotta say it is...|[0.90213519223812...|  0.0|       0.0|\n",
      "|i bought some of ...|[0.86795502804148...|  0.0|       0.0|\n",
      "|\"We breastfeed ou...|[0.89444355461525...|  1.0|       0.0|\n",
      "|MSG, Palm Oil, an...|[0.83221764638422...|  1.0|       0.0|\n",
      "|I can't believe t...|[0.83748831955996...|  1.0|       0.0|\n",
      "|I ordered these n...|[0.89571532028530...|  0.0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8436584609908302"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = lrModel.transform(testData)\n",
    "predictions_test.select(\"Text\",\"probability\",\"label\",\"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Test Accuracy\")\n",
    "evaluator.evaluate(predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using TF-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|  Status|           all_words|   stopwords_removed|                  TF|            features|label|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|Positive|[i, have, bought,...|[bought, several,...|(10000,[639,941,1...|(10000,[639,941,1...|  0.0|\n",
      "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|Negative|[product, arrived...|[product, arrived...|(10000,[1174,1648...|(10000,[1174,1648...|  1.0|\n",
      "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|Positive|[this, is, a, con...|[confection, arou...|(10000,[57,220,53...|(10000,[57,220,53...|  0.0|\n",
      "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|Negative|[if, you, are, lo...|[looking, secret,...|(10000,[295,1328,...|(10000,[295,1328,...|  1.0|\n",
      "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|Positive|[great, taffy, at...|[great, taffy, gr...|(10000,[1433,2836...|(10000,[1433,2836...|  0.0|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[Tokenizer_regex, Remove_stopwords, TF, idf, label_Target])\n",
    "pipelineFit = pipeline.fit(df)\n",
    "final_TF_idf = pipelineFit.transform(df)\n",
    "final_TF_idf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Count\n",
      "397945\n",
      "Test Data Count\n",
      "170509\n"
     ]
    }
   ],
   "source": [
    "(trainData_TF_idf, testData_TF_idf) = final_TF_idf.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Train Data Count\")\n",
    "print(str(trainData_TF_idf.count()))\n",
    "print(\"Test Data Count\")\n",
    "print(str(testData_TF_idf.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel_TF_idf = lr.fit(trainData_TF_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|                Text|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|I have bought sev...|[0.84096602345808...|  0.0|       0.0|\n",
      "|This is a very he...|[0.90071968211141...|  0.0|       0.0|\n",
      "|I fed this to my ...|[0.64773101606137...|  1.0|       0.0|\n",
      "|I have to admit, ...|[0.90509712302684...|  1.0|       0.0|\n",
      "|I love this noodl...|[0.88406138137306...|  0.0|       0.0|\n",
      "|That's pretty muc...|[0.78432301709773...|  0.0|       0.0|\n",
      "|These are very go...|[0.88654513634755...|  0.0|       0.0|\n",
      "|Very spicy packag...|[0.88865625530105...|  0.0|       0.0|\n",
      "|Delicious, awesom...|[0.87693259414089...|  0.0|       0.0|\n",
      "|Always good.  I d...|[0.88416923928414...|  0.0|       0.0|\n",
      "|It's quick to coo...|[0.89981801328921...|  0.0|       0.0|\n",
      "|the deliver is on...|[0.84272775631786...|  0.0|       0.0|\n",
      "|\"Love this soup! ...|[0.92351717304043...|  0.0|       0.0|\n",
      "|You get what you ...|[0.87142980714460...|  0.0|       0.0|\n",
      "|Good for a quick ...|[0.90601779469220...|  0.0|       0.0|\n",
      "|\"These are excell...|[0.94226517062630...|  0.0|       0.0|\n",
      "|I love this compa...|[0.87803354050488...|  0.0|       0.0|\n",
      "|One box of Shin N...|[0.79552951245746...|  1.0|       0.0|\n",
      "|Man this stuff is...|[0.88633031013009...|  0.0|       0.0|\n",
      "|One of my favorit...|[0.91202208651557...|  0.0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Train Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8345994322629462"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train_TF_idf = lrModel_TF_idf.transform(trainData_TF_idf)\n",
    "predictions_train_TF_idf.select(\"Text\",\"probability\",\"label\",\"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Train Accuracy\")\n",
    "evaluator.evaluate(predictions_train_TF_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|                Text|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|We have a 7 week ...|[0.92108889261432...|  0.0|       0.0|\n",
      "|The taste is grea...|[0.88331982423707...|  0.0|       0.0|\n",
      "|This is the best ...|[0.87865104449789...|  0.0|       0.0|\n",
      "|\"I don't see how ...|[0.89684987265900...|  0.0|       0.0|\n",
      "|I purchased this ...|[0.85834928537730...|  0.0|       0.0|\n",
      "|bought 2 package ...|[0.83377239630574...|  0.0|       0.0|\n",
      "|Shin Ramyun (sinc...|[0.89425060058872...|  0.0|       0.0|\n",
      "|I received Nongsh...|[0.88749141261992...|  0.0|       0.0|\n",
      "|These are perhaps...|[0.92511829089692...|  0.0|       0.0|\n",
      "|\"RECONSIDER THIS ...|[0.58239393230898...|  1.0|       0.0|\n",
      "|Excellent product...|[0.94056158309248...|  0.0|       0.0|\n",
      "|I really love thi...|[0.89759641803673...|  0.0|       0.0|\n",
      "|This is the most ...|[0.80052088245035...|  0.0|       0.0|\n",
      "|These noodles rem...|[0.96405088237814...|  0.0|       0.0|\n",
      "|As a Korean nativ...|[0.95766948721867...|  0.0|       0.0|\n",
      "|A delicious night...|[0.90720218617240...|  0.0|       0.0|\n",
      "|They're not for t...|[0.90757238574727...|  0.0|       0.0|\n",
      "|I gotta say it is...|[0.91414194953374...|  0.0|       0.0|\n",
      "|i bought some of ...|[0.87688421663264...|  0.0|       0.0|\n",
      "|\"We breastfeed ou...|[0.93320015369993...|  1.0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8331494767643901"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test_TF_idf = lrModel_TF_idf.transform(testData_TF_idf)\n",
    "predictions_test_TF_idf.select(\"Text\",\"probability\",\"label\",\"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Test Accuracy\")\n",
    "evaluator.evaluate(predictions_test_TF_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1)\n",
    "model_NB = nb.fit(trainData_TF_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|                Text|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|I have bought sev...|[0.99717079159599...|  0.0|       0.0|\n",
      "|This is a very he...|[0.99998259779735...|  0.0|       0.0|\n",
      "|I fed this to my ...|[5.02743064841116...|  1.0|       1.0|\n",
      "|I have to admit, ...|[1.0,4.2763142322...|  1.0|       0.0|\n",
      "|I love this noodl...|[0.99996223727718...|  0.0|       0.0|\n",
      "|That's pretty muc...|[0.99774860194651...|  0.0|       0.0|\n",
      "|These are very go...|[0.99999141563091...|  0.0|       0.0|\n",
      "|Very spicy packag...|[0.99999999900201...|  0.0|       0.0|\n",
      "|Delicious, awesom...|[0.99998537496198...|  0.0|       0.0|\n",
      "|Always good.  I d...|[0.99987755996523...|  0.0|       0.0|\n",
      "|It's quick to coo...|[0.99999999539160...|  0.0|       0.0|\n",
      "|the deliver is on...|[0.11735406584916...|  0.0|       1.0|\n",
      "|\"Love this soup! ...|[0.99999999999999...|  0.0|       0.0|\n",
      "|You get what you ...|[0.99999892781095...|  0.0|       0.0|\n",
      "|Good for a quick ...|[0.99999999951472...|  0.0|       0.0|\n",
      "|\"These are excell...|[1.0,4.3837103839...|  0.0|       0.0|\n",
      "|I love this compa...|[0.99993629770996...|  0.0|       0.0|\n",
      "|One box of Shin N...|[0.04011854746195...|  1.0|       1.0|\n",
      "|Man this stuff is...|[0.99997655354208...|  0.0|       0.0|\n",
      "|One of my favorit...|[1.0,1.2971448592...|  0.0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Train Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8689622001667336"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train_TF_idf_NB = model_NB.transform(trainData_TF_idf)\n",
    "predictions_train_TF_idf_NB.select(\"Text\",\"probability\",\"label\",\"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Train Accuracy\")\n",
    "evaluator.evaluate(predictions_train_TF_idf_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|                Text|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|We have a 7 week ...|[0.99999999994235...|  0.0|       0.0|\n",
      "|The taste is grea...|[0.99999939514935...|  0.0|       0.0|\n",
      "|This is the best ...|[0.99994602934327...|  0.0|       0.0|\n",
      "|\"I don't see how ...|[0.99999999990287...|  0.0|       0.0|\n",
      "|I purchased this ...|[0.94282840305928...|  0.0|       0.0|\n",
      "|bought 2 package ...|[0.96904960047021...|  0.0|       0.0|\n",
      "|Shin Ramyun (sinc...|[1.0,8.0513083484...|  0.0|       0.0|\n",
      "|I received Nongsh...|[0.99999999975747...|  0.0|       0.0|\n",
      "|These are perhaps...|[0.99999999999516...|  0.0|       0.0|\n",
      "|\"RECONSIDER THIS ...|[5.93077012108314...|  1.0|       1.0|\n",
      "|Excellent product...|[0.99999999999998...|  0.0|       0.0|\n",
      "|I really love thi...|[0.99999999340859...|  0.0|       0.0|\n",
      "|This is the most ...|[0.99963843621816...|  0.0|       0.0|\n",
      "|These noodles rem...|[1.0,3.2791064522...|  0.0|       0.0|\n",
      "|As a Korean nativ...|[1.0,1.1418257540...|  0.0|       0.0|\n",
      "|A delicious night...|[0.99999999618612...|  0.0|       0.0|\n",
      "|They're not for t...|[0.99999999721672...|  0.0|       0.0|\n",
      "|I gotta say it is...|[1.0,5.2037058260...|  0.0|       0.0|\n",
      "|i bought some of ...|[0.99989724673081...|  0.0|       0.0|\n",
      "|\"We breastfeed ou...|[1.0,2.2328395786...|  1.0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.864070763372753"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test_TF_idf_NB = model_NB.transform(testData_TF_idf)\n",
    "predictions_test_TF_idf_NB.select(\"Text\",\"probability\",\"label\",\"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Test Accuracy\")\n",
    "evaluator.evaluate(predictions_test_TF_idf_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting tokens to vectors using word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"stopwords_removed\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[Tokenizer_regex, Remove_stopwords,word2Vec ,label_Target])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "result = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Count\n",
      "397945\n",
      "Test Data Count\n",
      "170509\n"
     ]
    }
   ],
   "source": [
    "(trainData_wordvec, testData_wordvec) = result.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Train Data Count\")\n",
    "print(str(trainData_wordvec.count()))\n",
    "print(\"Test Data Count\")\n",
    "print(str(testData_wordvec.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression using word2vec vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel_wordvec = lr.fit(trainData_wordvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|                Text|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|I have bought sev...|[0.85185376495581...|  0.0|       0.0|\n",
      "|This is a very he...|[0.84052218477329...|  0.0|       0.0|\n",
      "|I fed this to my ...|[0.84115776266943...|  1.0|       0.0|\n",
      "|I have to admit, ...|[0.85996233748411...|  1.0|       0.0|\n",
      "|I love this noodl...|[0.84927127863579...|  0.0|       0.0|\n",
      "|That's pretty muc...|[0.86121998108371...|  0.0|       0.0|\n",
      "|These are very go...|[0.85872845914892...|  0.0|       0.0|\n",
      "|Very spicy packag...|[0.85258114510804...|  0.0|       0.0|\n",
      "|Delicious, awesom...|[0.86948081055635...|  0.0|       0.0|\n",
      "|Always good.  I d...|[0.85452297375336...|  0.0|       0.0|\n",
      "|It's quick to coo...|[0.85920451913633...|  0.0|       0.0|\n",
      "|the deliver is on...|[0.84240972313296...|  0.0|       0.0|\n",
      "|\"Love this soup! ...|[0.86022583754429...|  0.0|       0.0|\n",
      "|You get what you ...|[0.85023048861256...|  0.0|       0.0|\n",
      "|Good for a quick ...|[0.85911451147268...|  0.0|       0.0|\n",
      "|\"These are excell...|[0.85429240365412...|  0.0|       0.0|\n",
      "|I love this compa...|[0.85897924645492...|  0.0|       0.0|\n",
      "|One box of Shin N...|[0.83102055274811...|  1.0|       0.0|\n",
      "|Man this stuff is...|[0.86710699079947...|  0.0|       0.0|\n",
      "|One of my favorit...|[0.85750658498368...|  0.0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Train Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7843724492292861"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train_wordvec = lrModel_wordvec.transform(trainData_wordvec)\n",
    "predictions_train_wordvec.select(\"Text\",\"probability\",\"label\",\"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Train Accuracy\")\n",
    "evaluator.evaluate(predictions_train_wordvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|                Text|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|We have a 7 week ...|[0.83906671830046...|  0.0|       0.0|\n",
      "|The taste is grea...|[0.86395393187768...|  0.0|       0.0|\n",
      "|This is the best ...|[0.86651645424894...|  0.0|       0.0|\n",
      "|\"I don't see how ...|[0.86425862028939...|  0.0|       0.0|\n",
      "|I purchased this ...|[0.83986722801162...|  0.0|       0.0|\n",
      "|bought 2 package ...|[0.85047972607606...|  0.0|       0.0|\n",
      "|Shin Ramyun (sinc...|[0.84958657481014...|  0.0|       0.0|\n",
      "|I received Nongsh...|[0.84325885236766...|  0.0|       0.0|\n",
      "|These are perhaps...|[0.85517496969795...|  0.0|       0.0|\n",
      "|\"RECONSIDER THIS ...|[0.84094150559616...|  1.0|       0.0|\n",
      "|Excellent product...|[0.85327875832977...|  0.0|       0.0|\n",
      "|I really love thi...|[0.85281063755129...|  0.0|       0.0|\n",
      "|This is the most ...|[0.84504044070449...|  0.0|       0.0|\n",
      "|These noodles rem...|[0.85226451764990...|  0.0|       0.0|\n",
      "|As a Korean nativ...|[0.85558293709087...|  0.0|       0.0|\n",
      "|A delicious night...|[0.85710208818260...|  0.0|       0.0|\n",
      "|They're not for t...|[0.85336755682252...|  0.0|       0.0|\n",
      "|I gotta say it is...|[0.86084780930828...|  0.0|       0.0|\n",
      "|i bought some of ...|[0.84700781026749...|  0.0|       0.0|\n",
      "|\"We breastfeed ou...|[0.83881014675449...|  1.0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7847225175650496"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test_wordvec = lrModel_wordvec.transform(testData_wordvec)\n",
    "predictions_test_wordvec.select(\"Text\",\"probability\",\"label\",\"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print(\"Test Accuracy\")\n",
    "evaluator.evaluate(predictions_test_wordvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
